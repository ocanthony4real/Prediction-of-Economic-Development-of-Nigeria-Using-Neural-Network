# -*- coding: utf-8 -*-
"""Copy of Check_FY_Project_001.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlmEzTriUeehJJXpSZOZWBigptrorOmV
"""

#mount colab on gdrive
# from google.colab import drive
# drive.mount('/content/gdrive')

#import necessary libraries
import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD, Adam
from sklearn.preprocessing import MinMaxScaler
from streamlit_gsheets import GSheetsConnection

st.title('Prediction of Economic Development of Nigeria Using Neural Network')
#read dataset as excel file
# df = pd.read_excel('/content/gdrive/MyDrive/FinalYearProject/Datasets/West_AfricaEXCEL.xlsx')

conn = st.connection("gsheets", type=GSheetsConnection)

df = conn.read(spreadsheet=spreadsheet)
#df = pd.read_excel('/home/anthony/Downloads/West_AfricaEXCEL.xlsx')

"""# Data Pre-processing

"""

#remove 'Country Code' and 'Indicator Code' columns
df.drop(['Country Code', 'Indicator Code'], axis=1, inplace=True)

#filter only Nigeria's data
df = df[df["Country Name"] == "Nigeria"]

df.drop(["Country Name"], axis=1, inplace=True) #remove the 'Country Name' column
df = df.T #transpose the dataset

#reset the index
df = df.reset_index(drop=False)

#set the first row as the dataframe's column names
df.columns = df.iloc[0]

#remove the row index
df = df.drop(index=0)

#remove the 'Indicator Name' column
df.drop(columns=['Indicator Name'], axis=1, inplace=True)

df = df.dropna(axis=1, how='any') #remove columns that contains any fieal that's Nan

df = df.astype(float) #convert dataframe to float datatype

#get list of independent and target variables
# wanted_list = ['GNI (current US$)', 'Age dependency ratio, young (% of working-age population)', 'Merchandise imports (current US$)', 'Net primary income (Net income from abroad) (current US$)','Rural population', 'Urban population', 'Total reserves (includes gold, current US$)','Domestic credit to private sector by banks (% of GDP)','Inflation, consumer prices (annual %)']
wanted_list = ['GNI (current US$)', 'Age dependency ratio, young (% of working-age population)', 'Merchandise imports (current US$)', 'Multidimensional poverty index (scale 0-1)', 'Net primary income (Net income from abroad) (current US$)','Rural population', 'Urban population', 'Total reserves (includes gold, current US$)','Domestic credit to private sector by banks (% of GDP)']

#filter the dataframe to include only independent and target features
for i in df.columns:
  if i not in wanted_list:
    df = df.drop(columns=[i])
df

X = df.drop(columns=['GNI (current US$)']) # Features
y = df[['GNI (current US$)']] # Target variables

"""# Data Visualization"""

import seaborn as sns


correlation_matrix = df.corr()


sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')

"""# Model Design"""

LSTMUnits = 60

model = tf.keras.Sequential([
    tf.keras.layers.LSTM(LSTMUnits, activation="relu", input_shape=(4, 7)),
    tf.keras.layers.Dense(40, activation='relu'),
    tf.keras.layers.Dense(20, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1)])

model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mean_absolute_error')

model.summary()

"""# Data Splitting"""

#splitting and reshaping of training data
X_check_train = []
y_check_train = []
for i in range(4, 52):
  X_check_train.append(X[i-4:i])
  y_check_train.append(y.iloc[i-1][0])

X_train, y_train = np.array(X_check_train), np.array(y_check_train)

#splitting and reshaping of test data
X_check_test = []
y_check_test = []
for i in range(52, 64):
  X_check_test.append(X[i-4:i])
  y_check_test.append(y.iloc[i-1][0])

X_test, y_test = np.array(X_check_test), np.array(y_check_test)

print("x shape", X_train.shape, "Y shape", y_train.shape, "X test", X_test.shape, "Y test", y_test.shape)

"""# Data Normalization"""

from sklearn.preprocessing import StandardScaler

#normalization for independent features
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

#normalization for dependent features
scalers = MinMaxScaler()
y_train = scalers.fit_transform(y_train.reshape(-1, 1))
y_test = scalers.transform(y_test.reshape(-1, 1))

"""# Model Training"""

r = model.fit(X_train, y_train, epochs= 250, validation_data=(X_test, y_test))

plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()

"""# Prediction"""

scaled_pred = model.predict(X_test)

scaled_pred = scaled_pred.reshape(-1, 1).astype('float64')

#return predicted values to original scale

y_test = scalers.inverse_transform(y_test)
pred = scalers.inverse_transform(scaled_pred)

plt.plot(y_test, label='forecast target')
plt.plot(pred, label='forecast predictions')
plt.title('Comparison of Forcast Target & Prediction')
plt.rcParams["figure.figsize"] = (12,5)
plt.legend()

"""# Validaition Error"""

from sklearn.metrics import mean_absolute_percentage_error
MAPE = mean_absolute_percentage_error(y_test, pred)
MAPE

y_test

